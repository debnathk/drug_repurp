{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 16:58:14.490498: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-16 16:58:14.493586: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-16 16:58:14.799629: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-16 16:58:16.050983: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-16 16:58:56.689656: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset as dask dataframe\n",
    "\n",
    "root = \"/lustre/home/debnathk/dleps/code/DLEPS/reference_drug/\"\n",
    "# ssp_train_dask = dd.read_hdf(root + 'ssp_data_train.h5')\n",
    "\n",
    "import h5py\n",
    "import dask.array as da\n",
    "from dask import delayed\n",
    "\n",
    "# Define a delayed function to read a dataset from the HDF5 file\n",
    "@delayed\n",
    "def read_hdf5(path, dataset):\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        return f[dataset][:]\n",
    "\n",
    "# Use the delayed function to read the dataset\n",
    "ssp_train_dask = read_hdf5(root + 'ssp_data_train.h5', 'data')\n",
    "ssp_test_dask = read_hdf5(root + 'ssp_data_test.h5', 'data')\n",
    "\n",
    "# Convert the delayed object to a Dask array\n",
    "data = da.from_delayed(ssp_train_dask, shape=(44988, 207, 3072), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 213.15 GiB </td>\n",
       "                        <td> 213.15 GiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (44988, 207, 3072) </td>\n",
       "                        <td> (44988, 207, 3072) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 1 chunks in 2 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float64 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"166\" height=\"146\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"25\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 80.58823529411765,70.58823529411765 80.58823529411765,96.00085180870013 10.0,25.41261651458248\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"46\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"116\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"46\" y1=\"0\" x2=\"116\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 46.3913730637254,0.0 116.97960835784306,70.58823529411765 80.58823529411765,70.58823529411765\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"116\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"80\" y1=\"96\" x2=\"116\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"116\" y1=\"70\" x2=\"116\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"80.58823529411765,70.58823529411765 116.97960835784306,70.58823529411765 116.97960835784306,96.00085180870013 80.58823529411765,96.00085180870013\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"98.783922\" y=\"116.000852\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >3072</text>\n",
       "  <text x=\"136.979608\" y=\"83.294544\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,136.979608,83.294544)\">207</text>\n",
       "  <text x=\"35.294118\" y=\"80.706734\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,35.294118,80.706734)\">44988</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<from-value, shape=(44988, 207, 3072), dtype=float64, chunksize=(44988, 207, 3072), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 17:04:47.538859: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 228864393216 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "# Convert Dask arrays to TensorFlow datasets for training\n",
    "x_train = tf.data.Dataset.from_tensor_slices(data).batch(32)\n",
    "# x_test = tf.data.Dataset.from_tensor_slices(ssp_train_dask).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(Delayed('sample-f89b370c-dea0-400c-a7df-706de834280b'), dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(ssp_train_dask.sample(fraction=0.1))\n",
    "\n",
    "# Use TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Array' object has no attribute 'sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/lustre/home/debnathk/dleps/code/DLEPS/reference_drug/refDrug_vae.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/debnathk/dleps/code/DLEPS/reference_drug/refDrug_vae.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m data\u001b[39m.\u001b[39msample(fraction\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Array' object has no attribute 'sample'"
     ]
    }
   ],
   "source": [
    "data.sample(fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# root = \"/lustre/home/debnathk/dleps/code/DLEPS/reference_drug/\"\n",
    "# import h5py\n",
    "\n",
    "# h5f = h5py.File(root + 'ssp_data_train.h5', 'r')\n",
    "# ssp_train = h5f['data'][:]\n",
    "# h5f = h5py.File(root + 'ssp_data_test.h5', 'r')\n",
    "# ssp_test = h5f['data'][:]\n",
    "\n",
    "# print(ssp_train.shape)\n",
    "# print(ssp_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(ssp_train[:, 100, :].sum(axis=1) / 4000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a sampling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 207, 3072)]          0         []                            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 104, 32)              294944    ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 52, 64)               6208      ['conv1d[0][0]']              \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 3328)                 0         ['conv1d_1[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 16)                   53264     ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " z_mean (Dense)              (None, 56)                   952       ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " z_log_var (Dense)           (None, 56)                   952       ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " sampling (Sampling)         (None, 56)                   0         ['z_mean[0][0]',              \n",
      "                                                                     'z_log_var[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 356320 (1.36 MB)\n",
      "Trainable params: 356320 (1.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 56\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(207, 3072))\n",
    "# Add dense layer\n",
    "x = layers.Conv1D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv1D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 56)]              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                912       \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1, 16)             0         \n",
      "                                                                 \n",
      " conv1d_transpose (Conv1DTr  (None, 2, 64)             3136      \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " conv1d_transpose_1 (Conv1D  (None, 4, 32)             6176      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 635904)            82031616  \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 207, 3072)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82041840 (312.96 MB)\n",
      "Trainable params: 82041840 (312.96 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Decoder\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(16, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((1, 16))(x)\n",
    "x = layers.Conv1DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv1DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "decoder_outputs = layers.Dense(207 * 3072, activation=\"sigmoid\")(x)\n",
    "decoder_outputs = layers.Reshape((207, 3072))(decoder_outputs)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "decoder.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the VAE as a Model with a custom train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "class VAE(models.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(data, reconstruction))\n",
    "            kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "# Instantiate the VAE model\n",
    "vae = VAE(encoder=encoder, decoder=decoder)\n",
    "\n",
    "# Compile the model\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "num_epochs, batch_size = 3, 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate batches of data from our dask dataframe\n",
    "def dask_data_generator(df, fraction=0.1):\n",
    "    while True:\n",
    "        batch = df.sample(fraction=fraction)\n",
    "        X = batch.iloc[:, :]\n",
    "        yield X.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object dask_data_generator at 0x7f1a15bd0d60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_data_generator(ssp_train_dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3168275/1319909898.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = vae.fit_generator(generator= dask_data_generator(ssp_train_dask),\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/lustre/home/debnathk/dleps/code/DLEPS/reference_drug/refDrug_vae.ipynb Cell 17\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/debnathk/dleps/code/DLEPS/reference_drug/refDrug_vae.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/debnathk/dleps/code/DLEPS/reference_drug/refDrug_vae.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Assuming you've already trained the VAE using the code you provided\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/debnathk/dleps/code/DLEPS/reference_drug/refDrug_vae.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/debnathk/dleps/code/DLEPS/reference_drug/refDrug_vae.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Train the VAE model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/debnathk/dleps/code/DLEPS/reference_drug/refDrug_vae.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m history \u001b[39m=\u001b[39m vae\u001b[39m.\u001b[39mfit_generator(generator\u001b[39m=\u001b[39m dask_data_generator(ssp_train_dask),\n\u001b[1;32m      <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/debnathk/dleps/code/DLEPS/reference_drug/refDrug_vae.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m                             epochs\u001b[39m=\u001b[39mnum_epochs,\n\u001b[1;32m      <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/debnathk/dleps/code/DLEPS/reference_drug/refDrug_vae.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m                             steps_per_epoch\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/debnathk/dleps/code/DLEPS/reference_drug/refDrug_vae.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m                             validation_data\u001b[39m=\u001b[39mdask_data_generator(ssp_test_dask))\n\u001b[1;32m     <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/debnathk/dleps/code/DLEPS/reference_drug/refDrug_vae.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Plot the losses over epochs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/debnathk/dleps/code/DLEPS/reference_drug/refDrug_vae.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_losses\u001b[39m(history):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/engine/training.py:2913\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2901\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[1;32m   2902\u001b[0m \n\u001b[1;32m   2903\u001b[0m \u001b[39mDEPRECATED:\u001b[39;00m\n\u001b[1;32m   2904\u001b[0m \u001b[39m  `Model.fit` now supports generators, so there is no longer any need to\u001b[39;00m\n\u001b[1;32m   2905\u001b[0m \u001b[39m  use this endpoint.\u001b[39;00m\n\u001b[1;32m   2906\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2907\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   2908\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m`Model.fit_generator` is deprecated and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2909\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwill be removed in a future version. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2910\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2911\u001b[0m     stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   2912\u001b[0m )\n\u001b[0;32m-> 2913\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(\n\u001b[1;32m   2914\u001b[0m     generator,\n\u001b[1;32m   2915\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39msteps_per_epoch,\n\u001b[1;32m   2916\u001b[0m     epochs\u001b[39m=\u001b[39mepochs,\n\u001b[1;32m   2917\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m   2918\u001b[0m     callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[1;32m   2919\u001b[0m     validation_data\u001b[39m=\u001b[39mvalidation_data,\n\u001b[1;32m   2920\u001b[0m     validation_steps\u001b[39m=\u001b[39mvalidation_steps,\n\u001b[1;32m   2921\u001b[0m     validation_freq\u001b[39m=\u001b[39mvalidation_freq,\n\u001b[1;32m   2922\u001b[0m     class_weight\u001b[39m=\u001b[39mclass_weight,\n\u001b[1;32m   2923\u001b[0m     max_queue_size\u001b[39m=\u001b[39mmax_queue_size,\n\u001b[1;32m   2924\u001b[0m     workers\u001b[39m=\u001b[39mworkers,\n\u001b[1;32m   2925\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39muse_multiprocessing,\n\u001b[1;32m   2926\u001b[0m     shuffle\u001b[39m=\u001b[39mshuffle,\n\u001b[1;32m   2927\u001b[0m     initial_epoch\u001b[39m=\u001b[39minitial_epoch,\n\u001b[1;32m   2928\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;32m/lustre/home/debnathk/dleps/code/DLEPS/reference_drug/refDrug_vae.ipynb Cell 17\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/debnathk/dleps/code/DLEPS/reference_drug/refDrug_vae.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m batch \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39msample(fraction\u001b[39m=\u001b[39mfraction)\n\u001b[1;32m      <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/debnathk/dleps/code/DLEPS/reference_drug/refDrug_vae.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m X \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39miloc[:, :]\n\u001b[0;32m----> <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/debnathk/dleps/code/DLEPS/reference_drug/refDrug_vae.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39myield\u001b[39;00m X\u001b[39m.\u001b[39mcompute()\n",
      "File \u001b[0;32m/opt/anaconda3-2023.09-0/lib/python3.11/site-packages/dask/base.py:310\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    287\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \n\u001b[1;32m    289\u001b[0m \u001b[39m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39m    dask.compute\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     (result,) \u001b[39m=\u001b[39m compute(\u001b[39mself\u001b[39m, traverse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    311\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3-2023.09-0/lib/python3.11/site-packages/dask/base.py:595\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    592\u001b[0m     keys\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_keys__())\n\u001b[1;32m    593\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 595\u001b[0m results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    596\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/opt/anaconda3-2023.09-0/lib/python3.11/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool, multiprocessing\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[39m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[39m=\u001b[39m get_async(\n\u001b[1;32m     90\u001b[0m     pool\u001b[39m.\u001b[39msubmit,\n\u001b[1;32m     91\u001b[0m     pool\u001b[39m.\u001b[39m_max_workers,\n\u001b[1;32m     92\u001b[0m     dsk,\n\u001b[1;32m     93\u001b[0m     keys,\n\u001b[1;32m     94\u001b[0m     cache\u001b[39m=\u001b[39mcache,\n\u001b[1;32m     95\u001b[0m     get_id\u001b[39m=\u001b[39m_thread_get_id,\n\u001b[1;32m     96\u001b[0m     pack_exception\u001b[39m=\u001b[39mpack_exception,\n\u001b[1;32m     97\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[39m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m/opt/anaconda3-2023.09-0/lib/python3.11/site-packages/dask/local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m         _execute_task(task, data)  \u001b[39m# Re-execute locally\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         raise_exception(exc, tb)\n\u001b[1;32m    512\u001b[0m res, worker_id \u001b[39m=\u001b[39m loads(res_info)\n\u001b[1;32m    513\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mcache\u001b[39m\u001b[39m\"\u001b[39m][key] \u001b[39m=\u001b[39m res\n",
      "File \u001b[0;32m/opt/anaconda3-2023.09-0/lib/python3.11/site-packages/dask/local.py:319\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(exc, tb)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m exc\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[1;32m    318\u001b[0m     \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 319\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/opt/anaconda3-2023.09-0/lib/python3.11/site-packages/dask/local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     task, data \u001b[39m=\u001b[39m loads(task_info)\n\u001b[0;32m--> 224\u001b[0m     result \u001b[39m=\u001b[39m _execute_task(task, data)\n\u001b[1;32m    225\u001b[0m     \u001b[39mid\u001b[39m \u001b[39m=\u001b[39m get_id()\n\u001b[1;32m    226\u001b[0m     result \u001b[39m=\u001b[39m dumps((result, \u001b[39mid\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3-2023.09-0/lib/python3.11/site-packages/dask/core.py:121\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    117\u001b[0m     func, args \u001b[39m=\u001b[39m arg[\u001b[39m0\u001b[39m], arg[\u001b[39m1\u001b[39m:]\n\u001b[1;32m    118\u001b[0m     \u001b[39m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[39m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[39m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m(_execute_task(a, cache) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args))\n\u001b[1;32m    122\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    123\u001b[0m     \u001b[39mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m/opt/anaconda3-2023.09-0/lib/python3.11/site-packages/dask/utils.py:73\u001b[0m, in \u001b[0;36mapply\u001b[0;34m(func, args, kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Apply a function given its positional and keyword arguments.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[39mEquivalent to ``func(*args, **kwargs)``\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m>>> dsk = {'task-name': task}  # adds the task to a low level Dask task graph\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m kwargs:\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     74\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m/opt/anaconda3-2023.09-0/lib/python3.11/site-packages/dask/utils.py:1105\u001b[0m, in \u001b[0;36mmethodcaller.__call__\u001b[0;34m(self, _methodcaller__obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, __obj, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1105\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(__obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmethod)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'sample'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you've already trained the VAE using the code you provided\n",
    "\n",
    "# Train the VAE model\n",
    "history = vae.fit_generator(generator= dask_data_generator(ssp_train_dask),\n",
    "                            epochs=num_epochs,\n",
    "                            steps_per_epoch=100,\n",
    "                            validation_data=dask_data_generator(ssp_test_dask))\n",
    "\n",
    "# Plot the losses over epochs\n",
    "def plot_losses(history):\n",
    "    loss = history.history['loss']\n",
    "    reconstruction_loss = history.history['reconstruction_loss']\n",
    "    kl_loss = history.history['kl_loss']\n",
    "\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, loss, label='Total Loss')\n",
    "    plt.plot(epochs, reconstruction_loss, label='Reconstruction Loss')\n",
    "    plt.plot(epochs, kl_loss, label='KL Divergence Loss')\n",
    "\n",
    "    plt.title('Training Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(root + 'training_losses.png')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the losses\n",
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow\n",
    "\n",
    "keras.__version__\n",
    "tensorflow.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dleps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
